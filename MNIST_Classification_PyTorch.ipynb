{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "odZypv4VUH_z",
        "wE3l5KI_hx9X"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/PC3_Termination3/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnZier4ZSWNo",
        "outputId": "aaea66e7-655b-48e6-c08e-f6f4b38a0803"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rqqld1hwSI7l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_saving_path = '/content/drive/MyDrive/PC3_Termination3/MC_checkpoints/'"
      ],
      "metadata": {
        "id": "mM5HpAPqGMIt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "D4uvV1spPkU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 layered model\n",
        "\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Define a custom neural network with dropout\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(32*32, 128)\n",
        "#         self.fc2 = nn.Linear(128, 128)\n",
        "#         self.fc3 = nn.Linear(128, 64)\n",
        "#         self.fc4 = nn.Linear(64, 10)  # Output layer\n",
        "#         self.dropout = nn.Dropout(p=0.2)  # Dropout layer with a dropout probability of 0.2\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(-1, 32*32)  # Flatten the input\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)  # Apply dropout\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "#         x = self.dropout(x)  # Apply dropout\n",
        "#         x = torch.relu(self.fc3(x))\n",
        "#         x = self.dropout(x)  # Apply dropout\n",
        "#         x = self.fc4(x)  # Output layer\n",
        "#         return F.softmax(x, dim=1)  # Apply softmax activation\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(64*64, 256)  # Increase the input size and hidden units\n",
        "        self.fc2 = nn.Linear(256, 128)    # Adjust the hidden layer sizes\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)      # Output layer\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 64*64)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "# Initialize the network\n",
        "net = Net()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)  # lr = lr / gamma after every step_size number of epochs\n",
        "\n",
        "# Load the MNIST dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])   # convert data to tensors and transform it to have mean 0.5 and varience 0.5\n",
        "# transform = transforms.Compose([transforms.ToTensor()])   # convert data to tensors\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./MNIST_dataset', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./MNIST_dataset', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZZt0cx90iw99"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epoch = 0\n",
        "# Training loop\n",
        "for epoch in range(num_epoch):  # You can increase the number of epochs\n",
        "    running_loss = 0.0\n",
        "    running_loss_per_epoch = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_loss_per_epoch += loss.item()\n",
        "        if i % 100 == 99:  # Print loss every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Mini-Batch {i + 1}, Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    joblib.dump(net, f'{model_saving_path}/MC_epoch_{epoch+1}.pkl')\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss_per_epoch / 100:.4f}')\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "    # Access the current learning rate from the optimizer\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Print the current learning rate\n",
        "    print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbNUyJ75PfzV",
        "outputId": "0bda52de-822b-4a70-c89f-15152f9f5093"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loading_path = f'{model_saving_path}MC_epoch_10.pkl'\n",
        "net = joblib.load(loading_path)"
      ],
      "metadata": {
        "id": "s9tZSjETQA1m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GKQOVLdvP6BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)  # Assuming `net` is your trained model\n",
        "\n",
        "        # Get the index of the maximum output (predicted class)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the counts\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy on the test set: {}%'.format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjyRYnJyyYtX",
        "outputId": "73202764-baab-4e2b-b31e-2450284f1d1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 96.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MC Dropout"
      ],
      "metadata": {
        "id": "D-mxj7Y7UO2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.png')]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "#         image = Image.open(img_name)\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "#         return image\n",
        "\n",
        "# # loading images from file\n",
        "# generated_images_file_path = '/content/drive/MyDrive/PC3_Termination3/New_WGAN_output_images/'\n",
        "# generated_transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(),])\n",
        "# generated_dataset = CustomDataset(root_dir=generated_images_file_path, transform=generated_transform)\n",
        "# generated_batch_size = 64  # Adjust as needed\n",
        "# generated_dataloader = DataLoader(generated_dataset, batch_size=generated_batch_size, shuffle=False)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return idx, image  # Include the index (batch number) in the returned tuple\n",
        "\n",
        "# Loading images from file\n",
        "epoch_num = 2\n",
        "generated_images_file_path = f'/content/drive/MyDrive/PC3_Termination3/New_WGAN_epochwise_images/{epoch_num}'\n",
        "generated_transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(),])\n",
        "generated_dataset = CustomDataset(root_dir=generated_images_file_path, transform=generated_transform)\n",
        "generated_batch_size = 10000  # = number of images generated per epoch\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=generated_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "pxQzvYN3V6I7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To implement MC Dropout for uncertainty estimation, sample predictions multiple times with dropout enabled.\n",
        "\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "net.train()  # Set the network to training mode\n",
        "num_samples = 50  # Number of samples for MC Dropout\n",
        "stacked_tensor = torch.tensor([])\n",
        "# image_count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  # for inputs in generated_dataloader:\n",
        "  # for batch_number, (idx, inputs) in generated_dataloader:\n",
        "  for batch_number, (idx, inputs) in enumerate(generated_dataloader):\n",
        "    # image_count += generated_batch_size\n",
        "    # if image_count == 300:\n",
        "    if batch_number == epoch_num-1:\n",
        "      print(\"batch_number: \", batch_number)\n",
        "      print(\"index: \", idx)\n",
        "      predcitions_tensor = torch.tensor([])\n",
        "      for i in range(num_samples):\n",
        "          outputs = net(inputs)     # output.shape = torch.Size([batch_size, 10])\n",
        "          predcitions_tensor = torch.cat((predcitions_tensor, outputs.unsqueeze(2)), dim=2)\n",
        "      print(\"stacked_tensor_temp.shape: \", predcitions_tensor.shape)\n",
        "      stacked_tensor = torch.cat((stacked_tensor, predcitions_tensor), dim=0)\n",
        "      print(\"stacked_tensor.shape: \", stacked_tensor.shape)"
      ],
      "metadata": {
        "id": "CFbZlDfQOZsA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_number, (idx, inputs) in enumerate(generated_dataloader):\n",
        "  print(idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRBwzvxgsCnH",
        "outputId": "b7940eb2-16a7-49df-90db-36e2f9768d43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   0,    1,    2,  ..., 4811, 4812, 4813])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and variance of the predictions\n",
        "# predictions = torch.stack(predictions, dim=0)  # Stack predictions along a new dimension, predictions.shape = [num_samples x batch_size x 10], type(predictions) = torch.Tensor\n",
        "mean_prediction = torch.mean(stacked_tensor, dim=2)  # mean_prediction.shape = torch.Size([batch_size, 10])\n",
        "variance_prediction = torch.var(stacked_tensor, dim=2) # variance_prediction.shape = torch.Size([batch_size, 10])"
      ],
      "metadata": {
        "id": "pUC84FqAUovm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1 = torch.mean(mean_prediction)\n",
        "varience1 = torch.var(variance_prediction)\n",
        "print(mean1)\n",
        "print(varience1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKHdU42IUpR7",
        "outputId": "ce0d1953-54ed-442b-a7d9-7d61a9794e09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1000)\n",
            "tensor(0.0014)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum values and their indices along dim=1 (columns)\n",
        "max_values_mean, _ = torch.max(mean_prediction, dim=1)\n",
        "\n",
        "# Find the index of the maximum mean value along dim=1 (columns)\n",
        "max_mean_indices = torch.argmax(mean_prediction, dim=1)\n",
        "\n",
        "# Use the max_mean_indices to extract corresponding variance values\n",
        "max_variance_values = torch.gather(variance_prediction, dim=1, index=max_mean_indices.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "print(\"max_values_mean: \", max_values_mean)\n",
        "print(\"max_mean_indices: \", max_mean_indices)\n",
        "print(\"max_variance_values: \", max_variance_values)"
      ],
      "metadata": {
        "id": "WbeU5vcsnTPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfbe89f-f95b-48ea-e339-20f28d4232ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_values_mean:  tensor([0.9999, 0.9716, 0.9866,  ..., 0.9526, 0.7362, 0.8137])\n",
            "max_mean_indices:  tensor([3, 3, 3,  ..., 2, 3, 8])\n",
            "max_variance_values:  tensor([6.8288e-07, 1.4385e-02, 8.7862e-03,  ..., 3.3099e-02, 5.7250e-02,\n",
            "        5.6536e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_values_mean.mean())\n",
        "print(max_variance_values.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2P8EcUTntlw",
        "outputId": "68ada052-49e6-481a-e546-284e559bb8f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8236)\n",
            "tensor(0.0670)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "On all digits - correct and incorrect\n",
        "mean1\n",
        "variance1\n",
        "\n",
        "Accuracy on the test set: 96.88%\n",
        "\n",
        "For epoch1 and 2 combined\n",
        "tensor(0.1000)\n",
        "tensor(0.0013)\n",
        "\n",
        "\n",
        "Epoch1:\n",
        "tensor(0.1000)\n",
        "tensor(0.0015)\n",
        "\n",
        "Epoch2:\n",
        "tensor(0.1000)\n",
        "tensor(0.0011)\n",
        "'''"
      ],
      "metadata": {
        "id": "mcRXL4v5bAKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Accuracy on the test set: 96.88%\n",
        "\n",
        "For epoch1 and 2 combined\n",
        "tensor(0.8259)\n",
        "tensor(0.0656)\n",
        "\n",
        "Epoch1:\n",
        "tensor(0.7768)\n",
        "tensor(0.0841)\n",
        "\n",
        "Epoch2:\n",
        "tensor(0.8691)\n",
        "tensor(0.0495)\n",
        "'''"
      ],
      "metadata": {
        "id": "CvDLfInabAH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# old\n",
        "'''\n",
        "Mean and varience of true preds only:\n",
        "Generated image epoch 30:\n",
        "tensor(0.6512)\n",
        "tensor(0.1272)\n",
        "\n",
        "Generated image epoch 50:\n",
        "tensor(0.6730)\n",
        "tensor(0.1215)\n",
        "\n",
        "Generated image epoch 95:\n",
        "tensor(0.6295)\n",
        "tensor(0.1355)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sbnYbGgRn-Ru",
        "outputId": "882520c2-98e7-475a-9b10-4d96813ea2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMean and varience of true preds only:\\nGenerated image epoch 30:\\ntensor(0.6512)\\ntensor(0.1272)\\n\\nGenerated image epoch 50:\\n\\n\\nGenerated image epoch 95:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# old\n",
        "'''\n",
        "Mean and varience of all:\n",
        "Generated image epoch 30:\n",
        "tensor(0.1000)\n",
        "tensor(0.0024)\n",
        "\n",
        "Generated image epoch 50:\n",
        "tensor(0.1000)\n",
        "tensor(0.0022)\n",
        "\n",
        "Generated image epoch 95:\n",
        "tensor(0.1000)\n",
        "tensor(0.0029)\n",
        "'''"
      ],
      "metadata": {
        "id": "-Ehkz2X5kMXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Epoch = 1\n",
        "mean1 = tensor(-1.7371)\n",
        "varience1 = tensor(0.7028)\n",
        "\n",
        "Softmax\n",
        "tensor(0.1000)\n",
        "tensor(0.0001)\n",
        "\n",
        "\n",
        "Epoch = 3\n",
        "Accuracy on the test set: 95.91%\n",
        "tensor(-2.5869)\n",
        "tensor(3.4694)\n",
        "\n",
        "Softmax\n",
        "Accuracy on the test set: 94.72%\n",
        "tensor(0.1000)\n",
        "tensor(8.9567e-05)\n",
        "\n",
        "\n",
        "\n",
        "Epoch = 5\n",
        "Accuracy on the test set: 96.29%\n",
        "tensor(-3.4590)\n",
        "tensor(11.5309)\n",
        "\n",
        "Softmax\n",
        "Accuracy on the test set: 95.62%\n",
        "tensor(0.1000)\n",
        "tensor(1.2447e-05)\n",
        "\n",
        "\n",
        "Epoch = 10\n",
        "Accuracy on the test set: 96.46%\n",
        "Epoch 10, Loss: 14.0068\n",
        "tensor(0.1000)\n",
        "tensor(1.2360e-06)\n",
        "\n",
        "\n",
        "One extra layer\n",
        "Epoch = 30\n",
        "Accuracy on the test set: 96.64%\n",
        "Epoch 30, Loss: 13.9579\n",
        "tensor(0.1000)\n",
        "tensor(4.9995e-05)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "GS-MD8wA7Mk7",
        "outputId": "47406159-fedb-471a-ccc4-4b4757a17ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch = 1\\nmean1 = tensor(-1.7371)\\nvarience1 = tensor(0.7028)\\n\\nSoftmax\\ntensor(0.1000)\\ntensor(0.0001)\\n\\n\\nEpoch = 3\\nAccuracy on the test set: 95.91%\\ntensor(-2.5869)\\ntensor(3.4694)\\n\\nSoftmax\\nAccuracy on the test set: 94.72%\\ntensor(0.1000)\\ntensor(8.9567e-05)\\n\\n\\n\\nEpocch = 5\\nAccuracy on the test set: 96.29%\\ntensor(-3.4590)\\ntensor(11.5309)\\n\\nSoftmax\\nAccuracy on the test set: 95.62%\\ntensor(0.1000)\\ntensor(1.2447e-05)\\n\\n\\nEpoch = 10\\nAccuracy on the test set: 96.46%\\nEpoch 10, Loss: 14.0068\\ntensor(0.1000)\\ntensor(1.2360e-06)\\n\\n\\nEpoch = 30\\nAccuracy on the test set: 96.64%\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1wxR-W-OZpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ao-iWFusOZl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hncyJ4iAOZjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJwIH6RQOZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-YktDQL56nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZCIRQoV56i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MC Dropout Old"
      ],
      "metadata": {
        "id": "odZypv4VUH_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can use the trained network to make predictions.\n",
        "# To implement MC Dropout for uncertainty estimation, you can sample predictions multiple times with dropout enabled.\n",
        "# Here's an example of making predictions with MC Dropout:\n",
        "\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "net.train()  # Set the network to training mode\n",
        "num_samples = 100  # Number of samples for MC Dropout\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_samples):\n",
        "        outputs = net(inputs)     # output.shape = torch.Size([batch_size, 10])\n",
        "        predictions.append(outputs)"
      ],
      "metadata": {
        "id": "icDjjoXHyX2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and variance of the predictions\n",
        "predictions = torch.stack(predictions, dim=0)  # Stack predictions along a new dimension, predictions.shape = [num_samples x batch_size x 10], type(predictions) = torch.Tensor\n",
        "mean_prediction = torch.mean(predictions, dim=0)  # mean_prediction.shape = torch.Size([batch_size, 10])\n",
        "variance_prediction = torch.var(predictions, dim=0) # variance_prediction.shape = torch.Size([batch_size, 10])"
      ],
      "metadata": {
        "id": "0gwcOVO1283U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1 = torch.mean(mean_prediction)\n",
        "varience1 = torch.var(variance_prediction)\n",
        "print(mean1)\n",
        "print(varience1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqTPjBF16UGy",
        "outputId": "a7ea4845-4d60-47ab-bf38-f115417b00dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1000)\n",
            "tensor(3.9906e-06)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old"
      ],
      "metadata": {
        "id": "wE3l5KI_hx9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 layered model\n",
        "\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Define a custom neural network with dropout\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)  # Output layer\n",
        "        self.dropout = nn.Dropout(p=0.2)  # Dropout layer with a dropout probability of 0.2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc4(x)  # Output layer\n",
        "        return F.softmax(x, dim=1)  # Apply softmax activation\n",
        "\n",
        "# Initialize the network\n",
        "net = Net()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)  # lr = lr / gamma after every step_size number of epochs\n",
        "\n",
        "# Load the MNIST dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])   # convert data to tensors and transform it to have mean 0.5 and varience 0.5\n",
        "# transform = transforms.Compose([transforms.ToTensor()])   # convert data to tensors\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epoch = 30\n",
        "# Training loop\n",
        "for epoch in range(num_epoch):  # You can increase the number of epochs\n",
        "    running_loss = 0.0\n",
        "    running_loss_per_epoch = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_loss_per_epoch += loss.item()\n",
        "        if i % 100 == 99:  # Print loss every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Mini-Batch {i + 1}, Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    joblib.dump(net, f'{model_saving_path}/net_epoch_{epoch+1}.pkl')\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss_per_epoch / 100:.4f}')\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "    # Access the current learning rate from the optimizer\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Print the current learning rate\n",
        "    print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "DR858G1U56fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 layered model\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Define a custom neural network with dropout\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)  # Output layer\n",
        "        self.dropout = nn.Dropout(p=0.2)  # Dropout layer with a dropout probability of 0.2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return F.softmax(x, dim=1)  # Apply softmax activation\n",
        "\n",
        "# Initialize the network\n",
        "net = Net()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)  # lr = lr / gamma after every step_size number of epochs\n",
        "\n",
        "# Load the MNIST dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])   # convert data to tensors and transform it to have mean 0.5 and varience 0.5\n",
        "# transform = transforms.Compose([transforms.ToTensor()])   # convert data to tensors\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epoch = 10\n",
        "# Training loop\n",
        "for epoch in range(num_epoch):  # You can increase the number of epochs\n",
        "    running_loss = 0.0\n",
        "    running_loss_per_epoch = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_loss_per_epoch += loss.item()\n",
        "        if i % 100 == 99:  # Print loss every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Mini-Batch {i + 1}, Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    joblib.dump(net, f'{model_saving_path}/net_epoch_{epoch+1}.pkl')\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss_per_epoch / 100:.4f}')\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "    # Access the current learning rate from the optimizer\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Print the current learning rate\n",
        "    print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "1YVH35yNyXp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Older version\n",
        "\n",
        "# Now you can use the trained network to make predictions.\n",
        "# To implement MC Dropout for uncertainty estimation, you can sample predictions multiple times with dropout enabled.\n",
        "# Here's an example of making predictions with MC Dropout:\n",
        "\n",
        "# Set a random seed for NumPy for CPU operations\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set a random seed for PyTorch for GPU (if available) and CPU operations\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "net.train()  # Set the network to training mode\n",
        "num_samples = 50  # Number of samples for MC Dropout\n",
        "\n",
        "# predictions = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   for inputs in generated_dataloader:\n",
        "#     print(inputs.shape)               # print(inputs.shape) = torch.Size([batch_size, 1, 32, 32])\n",
        "#     for i in range(num_samples):\n",
        "#         outputs = net(inputs)     # output.shape = torch.Size([batch_size, 10])\n",
        "#         print(outputs.shape)\n",
        "#         predictions.append(outputs)\n",
        "\n",
        "# # predcitions_tensor = torch.tensor([])\n",
        "# stacked_tensor = torch.tensor([])\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   for inputs in generated_dataloader:\n",
        "#     # print(inputs.shape)               # print(inputs.shape) = torch.Size([batch_size, 1, 32, 32])\n",
        "#     # for i in range(num_samples):\n",
        "#     predictions_lst = []\n",
        "#     for i in range(num_samples):\n",
        "#         outputs = net(inputs)     # output.shape = torch.Size([batch_size, 10])\n",
        "#         # print(outputs.shape)\n",
        "#         predictions_lst.append(outputs)\n",
        "#     stacked_tensor_temp = torch.stack(predictions_lst, dim=2)\n",
        "#     print(\"stacked_tensor_temp.shape\", stacked_tensor_temp.shape)\n",
        "#     # if stacked_tensor is None:\n",
        "#     #     stacked_tensor = stacked_tensor_temp\n",
        "#     # else:\n",
        "#     #     # Stack the individual tensor on top of the existing stacked_tensor along dimension 0\n",
        "#     #     stacked_tensor = torch.cat((stacked_tensor, stacked_tensor_temp), dim=0)\n",
        "\n",
        "#     stacked_tensor = torch.cat((stacked_tensor, stacked_tensor_temp), dim=0)\n",
        "#     print(stacked_tensor.shape)\n",
        "\n",
        "\n",
        "\n",
        "stacked_tensor = torch.tensor([])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs in generated_dataloader:\n",
        "    predcitions_tensor = torch.tensor([])\n",
        "    for i in range(num_samples):\n",
        "        outputs = net(inputs)     # output.shape = torch.Size([batch_size, 10])\n",
        "        predcitions_tensor = torch.cat((predcitions_tensor, outputs.unsqueeze(2)), dim=2)\n",
        "    print(\"stacked_tensor_temp.shape\", predcitions_tensor.shape)\n",
        "    stacked_tensor = torch.cat((stacked_tensor, predcitions_tensor), dim=0)\n",
        "    print(stacked_tensor.shape)"
      ],
      "metadata": {
        "id": "JIaMDQBsyXnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZgy6QFayXk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uA96r2cAyXdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a custom neural network with dropout\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(28*28, 128)\n",
        "#         self.fc2 = nn.Linear(128, 64)\n",
        "#         self.fc3 = nn.Linear(64, 10)  # Output layer\n",
        "#         self.dropout = nn.Dropout(p=0.5)  # Dropout layer with a dropout probability of 0.5\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(-1, 28*28)  # Flatten the input\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)  # Apply dropout\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "#         x = self.dropout(x)  # Apply dropout\n",
        "#         x = self.fc3(x)  # Output layer\n",
        "#         return x\n",
        "\n",
        "# # Initialize the network\n",
        "# net = Net()\n",
        "\n",
        "# # Define loss and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# # Load the MNIST dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(5):  # You can increase the number of epochs\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, labels = data\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 100 == 99:  # Print every 100 mini-batches\n",
        "#             print(f'Epoch {epoch + 1}, Mini-Batch {i + 1}, Loss: {running_loss / 100:.4f}')\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')\n",
        "\n",
        "# # Now you can use the trained network to make predictions.\n",
        "# # To implement MC Dropout for uncertainty estimation, you can sample predictions multiple times with dropout enabled.\n",
        "# # Here's an example of making predictions with MC Dropout:\n",
        "\n",
        "# # Now, let's test the model on the MNIST test dataset.\n",
        "# net.eval()  # Set the network to evaluation mode\n",
        "\n",
        "# testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         inputs, labels = data\n",
        "#         outputs = net(inputs)\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f'Accuracy on test dataset: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "tTkJFrMXqzAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmmIT96gqy87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPjVCRiXqy5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a CNN model\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "#         self.fc1 = nn.Linear(64*5*5, 128)\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     x = torch.relu(self.conv1(x))\n",
        "#     #     x = torch.relu(self.conv2(x))\n",
        "#     #     x = x.view(-1, 64*5*5)\n",
        "#     #     x = torch.relu(self.fc1(x))\n",
        "#     #     x = self.fc2(x)\n",
        "#     #     return x\n",
        "#     # def forward(self, x):\n",
        "#     #     x = torch.relu(self.conv1(x))\n",
        "#     #     x = torch.relu(self.conv2(x))\n",
        "#     #     # Calculate the correct size based on the output feature map dimensions\n",
        "#     #     x = x.view(x.size(0), -1)\n",
        "#     #     x = torch.relu(self.fc1(x))\n",
        "#     #     x = self.fc2(x)\n",
        "#     #     return x\n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.conv1(x))\n",
        "#         x = torch.relu(self.conv2(x))\n",
        "\n",
        "#         # Flatten the tensor, assuming correct output size from conv2\n",
        "#         x = x.view(x.size(0), -1)\n",
        "\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# # Load the MNIST dataset and apply transformations\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# # Initialize the model and optimizer\n",
        "# model = Net()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(5):\n",
        "#     for images, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "# # Load the test dataset\n",
        "# test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# # Evaluate the model on test data\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# predictions = []\n",
        "# confidence_interval = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         confidence = torch.softmax(outputs, dim=1)\n",
        "#         max_confidence = torch.max(confidence, dim=1)\n",
        "#         predictions.extend(predicted.numpy())\n",
        "#         confidence_interval.extend(max_confidence.values.numpy())\n",
        "\n",
        "# print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# # Display results for a specific test example (e.g., index 0)\n",
        "# print(f\"Predicted Digit: {predictions[0]}\")\n",
        "# print(f\"Confidence Interval: {confidence_interval[0]:.2f}\")\n"
      ],
      "metadata": {
        "id": "yu5c8bttSO6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7fExLVCSkdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdM3jQ1uTH23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}