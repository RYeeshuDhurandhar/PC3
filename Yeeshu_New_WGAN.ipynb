{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/PC3_Termination3/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yclh8Z3MOMVf",
        "outputId": "89eac5f7-b98b-4cea-cd5b-732943efd0af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import grad as torch_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import imageio\n",
        "import time\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "import os"
      ],
      "metadata": {
        "id": "81vlngR7OKUq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9WGtXfs71Ck",
        "outputId": "7f8af002-ddd0-41d6-b4d5-b99c35364f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "\n",
            "Epoch 1\n",
            "Iteration 1\n",
            "D: 3.461726188659668\n",
            "Gradient penalty: 3.525785207748413\n",
            "Gradient norm: 1.4071855545043945\n",
            "Iteration 501\n",
            "D: 0.5969372987747192\n",
            "Gradient penalty: 1.0128111839294434\n",
            "Gradient norm: 0.7579784989356995\n",
            "G: -0.2523704767227173\n",
            "Folder './New_WGAN_epochwise_images/1' created successfully.\n",
            "\n",
            "Epoch 2\n",
            "Iteration 1\n",
            "D: -0.40593409538269043\n",
            "Gradient penalty: 0.13966047763824463\n",
            "Gradient norm: 0.9980427026748657\n",
            "G: -0.3058271110057831\n",
            "Iteration 501\n",
            "D: -0.4012530446052551\n",
            "Gradient penalty: 0.1361970156431198\n",
            "Gradient norm: 0.9160897135734558\n",
            "G: -0.3603123426437378\n",
            "Folder './New_WGAN_epochwise_images/2' created successfully.\n",
            "\n",
            "Epoch 3\n",
            "Iteration 1\n",
            "D: -0.04974338412284851\n",
            "Gradient penalty: 0.21470561623573303\n",
            "Gradient norm: 0.8883152008056641\n",
            "G: -0.6096866726875305\n",
            "Iteration 501\n",
            "D: -0.19506779313087463\n",
            "Gradient penalty: 0.0894557535648346\n",
            "Gradient norm: 1.0058560371398926\n",
            "G: -0.6397234201431274\n",
            "Folder './New_WGAN_epochwise_images/3' created successfully.\n",
            "\n",
            "Epoch 4\n",
            "Iteration 1\n",
            "D: 0.06998938322067261\n",
            "Gradient penalty: 0.39846283197402954\n",
            "Gradient norm: 0.8434362411499023\n",
            "G: -0.2634931206703186\n",
            "Iteration 501\n",
            "D: -0.17281079292297363\n",
            "Gradient penalty: 0.42073071002960205\n",
            "Gradient norm: 0.8254441618919373\n",
            "G: -0.40265795588493347\n",
            "Folder './New_WGAN_epochwise_images/4' created successfully.\n",
            "\n",
            "Epoch 5\n",
            "Iteration 1\n",
            "D: -0.226056307554245\n",
            "Gradient penalty: 0.17211589217185974\n",
            "Gradient norm: 0.8951861262321472\n",
            "G: -0.3580910563468933\n",
            "Iteration 501\n",
            "D: -0.3104243278503418\n",
            "Gradient penalty: 0.13122546672821045\n",
            "Gradient norm: 0.9209519624710083\n",
            "G: -0.0824543684720993\n",
            "Folder './New_WGAN_epochwise_images/5' created successfully.\n",
            "\n",
            "Epoch 6\n",
            "Iteration 1\n",
            "D: -0.18852195143699646\n",
            "Gradient penalty: 0.2704562842845917\n",
            "Gradient norm: 0.8572012186050415\n",
            "G: -0.29146185517311096\n",
            "Iteration 501\n",
            "D: -0.6882280111312866\n",
            "Gradient penalty: 0.09220109134912491\n",
            "Gradient norm: 0.9489182233810425\n",
            "G: -0.0634985864162445\n",
            "Folder './New_WGAN_epochwise_images/6' created successfully.\n",
            "\n",
            "Epoch 7\n",
            "Iteration 1\n",
            "D: -0.6879273653030396\n",
            "Gradient penalty: 0.05440950393676758\n",
            "Gradient norm: 1.0327959060668945\n",
            "G: -0.17415326833724976\n",
            "Iteration 501\n",
            "D: -0.8274345993995667\n",
            "Gradient penalty: 0.13028885424137115\n",
            "Gradient norm: 1.097182273864746\n",
            "G: -0.024557020515203476\n",
            "Folder './New_WGAN_epochwise_images/7' created successfully.\n",
            "\n",
            "Epoch 8\n",
            "Iteration 1\n",
            "D: -0.5135591626167297\n",
            "Gradient penalty: 0.07264731824398041\n",
            "Gradient norm: 1.0628621578216553\n",
            "G: -0.17565685510635376\n",
            "Iteration 501\n",
            "D: -0.48483017086982727\n",
            "Gradient penalty: 0.14397647976875305\n",
            "Gradient norm: 0.9049758911132812\n",
            "G: -0.0967922955751419\n",
            "Folder './New_WGAN_epochwise_images/8' created successfully.\n",
            "\n",
            "Epoch 9\n",
            "Iteration 1\n",
            "D: -0.6597140431404114\n",
            "Gradient penalty: 0.1506432443857193\n",
            "Gradient norm: 1.109751582145691\n",
            "G: -0.05374531447887421\n",
            "Iteration 501\n",
            "D: -0.7265176773071289\n",
            "Gradient penalty: 0.1386556625366211\n",
            "Gradient norm: 1.0972729921340942\n",
            "G: -0.06288161873817444\n",
            "Folder './New_WGAN_epochwise_images/9' created successfully.\n",
            "\n",
            "Epoch 10\n",
            "Iteration 1\n",
            "D: 6.122665882110596\n",
            "Gradient penalty: 6.401447296142578\n",
            "Gradient norm: 0.20470455288887024\n",
            "G: -0.0891944169998169\n",
            "Iteration 501\n",
            "D: -0.5111796855926514\n",
            "Gradient penalty: 0.3743091821670532\n",
            "Gradient norm: 0.8508259057998657\n",
            "G: -0.4572749435901642\n",
            "Folder './New_WGAN_epochwise_images/10' created successfully.\n",
            "Training Completed. It took:  2083.721722841263\n"
          ]
        }
      ],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "ngpu=1\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "    def sample_latent(self, num_samples):\n",
        "        return torch.randn((num_samples, 100, 1, 1))\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, generator, discriminator, gen_optimizer, dis_optimizer,\n",
        "                 gp_weight=10, critic_iterations=5, print_every=500,\n",
        "                 use_cuda=False, output_dir='./output'):\n",
        "        self.G = generator\n",
        "        self.G_opt = gen_optimizer\n",
        "        self.D = discriminator\n",
        "        self.D_opt = dis_optimizer\n",
        "        self.losses = {'G': [], 'D': [], 'GP': [], 'gradient_norm': []}\n",
        "        self.num_steps = 0\n",
        "        self.use_cuda = use_cuda\n",
        "        self.gp_weight = gp_weight\n",
        "        self.critic_iterations = critic_iterations\n",
        "        self.print_every = print_every\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "\n",
        "    def _critic_train_iteration(self, data):\n",
        "\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate probabilities on real and generated data\n",
        "        data = Variable(data)\n",
        "        if self.use_cuda:\n",
        "            data = data.cuda()\n",
        "        d_real = self.D(data)\n",
        "        d_generated = self.D(generated_data)\n",
        "\n",
        "        # Get gradient penalty\n",
        "        gradient_penalty = self._gradient_penalty(data, generated_data)\n",
        "        self.losses['GP'].append(gradient_penalty.item())\n",
        "\n",
        "        # Create total loss and optimize\n",
        "        self.D_opt.zero_grad()\n",
        "        d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n",
        "        d_loss.backward()\n",
        "        self.D_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['D'].append(d_loss.item())\n",
        "\n",
        "    def _generator_train_iteration(self, data):\n",
        "\n",
        "        self.G_opt.zero_grad()\n",
        "\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate loss and optimize\n",
        "        d_generated = self.D(generated_data)\n",
        "        g_loss = - d_generated.mean()\n",
        "        g_loss.backward()\n",
        "        self.G_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['G'].append(g_loss.item())\n",
        "\n",
        "    def _gradient_penalty(self, real_data, generated_data):\n",
        "        batch_size = real_data.size()[0]\n",
        "\n",
        "        # Calculate interpolation\n",
        "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
        "        alpha = alpha.expand_as(real_data)\n",
        "        if self.use_cuda:\n",
        "            alpha = alpha.cuda()\n",
        "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        if self.use_cuda:\n",
        "            interpolated = interpolated.cuda()\n",
        "\n",
        "        # Calculate probability of interpolated examples\n",
        "        prob_interpolated = self.D(interpolated)\n",
        "\n",
        "        # Calculate gradients of probabilities with respect to examples\n",
        "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(\n",
        "                               prob_interpolated.size()),\n",
        "                               create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Gradients have shape (batch_size, num_channels, img_width, img_height), so flatten to easily take norm per example in batch\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().item())\n",
        "\n",
        "        # Derivatives of the gradient close to 0 can cause problems because of the square root, so manually calculate norm and add epsilon\n",
        "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "        # Return gradient penalty\n",
        "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
        "\n",
        "    def _train_epoch(self, data_loader):\n",
        "        for i, data in enumerate(data_loader):\n",
        "            self.num_steps += 1\n",
        "            self._critic_train_iteration(data[0])\n",
        "            # Only update generator every |critic_iterations| iterations\n",
        "            if self.num_steps % self.critic_iterations == 0:\n",
        "                self._generator_train_iteration(data[0])\n",
        "\n",
        "            if i % self.print_every == 0:\n",
        "                print(\"Iteration {}\".format(i + 1))\n",
        "                print(\"D: {}\".format(self.losses['D'][-1]))\n",
        "                print(\"Gradient penalty: {}\".format(self.losses['GP'][-1]))\n",
        "                print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
        "                if self.num_steps > self.critic_iterations:\n",
        "                    print(\"G: {}\".format(self.losses['G'][-1]))\n",
        "\n",
        "    def train(self, data_loader, epochs, save_training_images=True):\n",
        "        for epoch in range(epochs):\n",
        "            print(\"\\nEpoch {}\".format(epoch + 1))\n",
        "            self._train_epoch(data_loader)\n",
        "\n",
        "            if save_training_images:\n",
        "              self.save_training_images(epoch + 1)\n",
        "\n",
        "            # if (epoch + 1) % 1 == 0:\n",
        "            #     generator_path = f'./models_FP32_CIFAR10/generator_epoch_{epoch + 1}.pkl'\n",
        "            #     discriminator_path = f'./models_FP32_CIFAR10/discriminator_epoch_{epoch + 1}.pkl'\n",
        "            #     joblib.dump(self.G, generator_path)\n",
        "            #     joblib.dump(self.D, discriminator_path)\n",
        "            generator_path = f'./New_WGAN_checkpoints/generator_epoch_{epoch + 1}.pkl'\n",
        "            discriminator_path = f'./New_WGAN_checkpoints/discriminator_epoch_{epoch + 1}.pkl'\n",
        "            joblib.dump(self.G, generator_path)\n",
        "            joblib.dump(self.D, discriminator_path)\n",
        "\n",
        "    def sample_generator(self, num_samples):\n",
        "        latent_samples = Variable(self.G.sample_latent(num_samples))\n",
        "        if self.use_cuda:\n",
        "            latent_samples = latent_samples.cuda()\n",
        "        generated_data = self.G(latent_samples)\n",
        "        return generated_data\n",
        "\n",
        "    # def save_training_images(self, epoch):\n",
        "    #     # Save batch of training images\n",
        "    #     fixed_latents = Variable(self.G.sample_latent(64))\n",
        "    #     if self.use_cuda:\n",
        "    #         fixed_latents = fixed_latents.cuda()\n",
        "    #     img_grid = make_grid(self.G(fixed_latents).cpu().data)\n",
        "    #     img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "    #     imageio.imwrite('{}/training_images_epoch_{}.png'.format(self.output_dir, epoch), (img_grid * 255).astype(np.uint8))\n",
        "    def save_training_images(self, epoch):\n",
        "        # Save individual training images\n",
        "        # Generate a batch of fixed random latent vectors using the generator's sample_latent method.\n",
        "        num_img_to_generate = 10000\n",
        "        fixed_latents = Variable(self.G.sample_latent(num_img_to_generate))\n",
        "\n",
        "        # If GPU (CUDA) is used, move the latent vectors to the GPU.\n",
        "        if self.use_cuda:\n",
        "            fixed_latents = fixed_latents.cuda()\n",
        "\n",
        "        # Specify the path of the folder you want to create\n",
        "        folder_path = f'{self.output_dir}/{epoch}'\n",
        "\n",
        "        # Check if the folder exists, and if not, create it\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "            print(f\"Folder '{folder_path}' created successfully.\")\n",
        "\n",
        "        # Generate individual images from the fixed_latents using the generator (self.G).\n",
        "        for i, latent in enumerate(fixed_latents):\n",
        "            # Generate an image from the current latent vector.\n",
        "            generated_image = self.G(latent.unsqueeze(0))  # Unsqueeze to add a batch dimension\n",
        "\n",
        "            # Convert the generated image from GPU to CPU and get the data.\n",
        "            generated_image_cpu = generated_image.cpu().data\n",
        "\n",
        "            # Save the individual image as a PNG file with a filename that includes the epoch and index.\n",
        "            imageio.imwrite(f'{folder_path}/training_image_epoch_{epoch}_{i}.png', (generated_image_cpu.squeeze().numpy() * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "# Function to get data loaders for various datasets\n",
        "def get_data_loaders(batch_size=128):\n",
        "\n",
        "  transform = transforms.Compose([transforms.Resize(64), transforms.CenterCrop(64), transforms.ToTensor(), transforms.Normalize((0), (1)),])\n",
        "  train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "  test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return train_loader, test_loader\n",
        "\n",
        "# Device Configuration\n",
        "start_time = time.time()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# torch.backends.cudnn.enabled = False\n",
        "print(device)\n",
        "\n",
        "# # Create the \"models\" directory if it doesn't exist\n",
        "# if not os.path.exists('./models_FP32_CIFAR10'):\n",
        "#     os.makedirs('./models_FP32_CIFAR10')\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator(ngpu).to(device)\n",
        "generator.apply(weights_init)\n",
        "discriminator = Discriminator(ngpu).to(device)\n",
        "discriminator.apply(weights_init)\n",
        "\n",
        "# Initialize optimizers\n",
        "G_optimizer = torch.optim.Adam(generator.parameters(), lr=5e-4, betas=(.5, .999))\n",
        "D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(.5, .999))\n",
        "\n",
        "# Set up trainer\n",
        "output_directory = './New_WGAN_epochwise_images'\n",
        "trainer = Trainer(generator, discriminator, G_optimizer, D_optimizer, use_cuda=torch.cuda.is_available(), output_dir=output_directory)\n",
        "\n",
        "num_epoch = 10\n",
        "data_loader, _ = get_data_loaders(batch_size=64)\n",
        "trainer.train(data_loader, epochs=num_epoch, save_training_images=True)\n",
        "\n",
        "print(\"Training Completed. It took: \", time.time() - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1TcvJ2F6Qzfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfHjgG9wQzcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5SyjhfFWEQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxUhjYRgWENb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5rmxOLDQzUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLU-ITLdbl1S",
        "outputId": "2e5b990e-6eea-40e6-d878-773236298a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output_new.zip output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4twmFCfbsZI",
        "outputId": "89509ce4-7f43-4898-eb5e-57bfe859fd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: output/ (stored 0%)\n",
            "  adding: output/training_images_epoch_16.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_2.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_9.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_4.png (deflated 3%)\n",
            "  adding: output/training_images_epoch_11.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_15.png (deflated 1%)\n",
            "  adding: output/training_images_epoch_18.png (deflated 1%)\n",
            "  adding: output/training_images_epoch_1.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_7.png (deflated 3%)\n",
            "  adding: output/training_images_epoch_12.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_14.png (deflated 1%)\n",
            "  adding: output/training_images_epoch_17.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_10.png (deflated 2%)\n",
            "  adding: output/training_images_epoch_8.png (deflated 3%)\n",
            "  adding: output/training_images_epoch_6.png (deflated 3%)\n",
            "  adding: output/training_images_epoch_19.png (deflated 1%)\n",
            "  adding: output/training_images_epoch_3.png (deflated 3%)\n",
            "  adding: output/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: output/training_images_epoch_13.png (deflated 1%)\n",
            "  adding: output/training_images_epoch_5.png (deflated 3%)\n",
            "  adding: output/training_images_epoch_20.png (deflated 2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('output_new.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g6tosoZobxyg",
        "outputId": "51efda0a-af5f-4a7c-ed9a-b5d38bfa35ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8db6de93-0d2b-49ae-8ba2-58b83ca45301\", \"output_new.zip\", 3704129)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}